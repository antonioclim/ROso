#!/usr/bin/env python3
"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
npu_benchmark.py - ComparaÈ›ie inferenÈ›Äƒ NPU vs CPU vs GPU
SO Curs 18supp - Integrarea NPU Ã®n Sistemele de Operare

Requires: pip install openvino onnxruntime numpy
Optional: pip install onnxruntime-gpu (pentru CUDA)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

from __future__ import annotations

import sys
import time
from pathlib import Path
from typing import Any

import numpy as np

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONSTANTE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

MODEL_URL = "https://github.com/onnx/models/raw/main/validated/vision/classification/mobilenet/model/mobilenetv2-7.onnx"
MODEL_PATH = Path("mobilenetv2-7.onnx")
DEFAULT_ITERATIONS = 100
WARMUP_ITERATIONS = 10


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DOWNLOAD MODEL
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def download_model() -> str:
    """Download a test model (MobileNetV2) if not present."""
    if MODEL_PATH.exists():
        return str(MODEL_PATH)

    print("Downloading MobileNetV2 model...")
    try:
        import urllib.request
        urllib.request.urlretrieve(MODEL_URL, MODEL_PATH)
        print(f"  Downloaded: {MODEL_PATH}")
    except Exception as e:
        print(f"  Error downloading: {e}")
        print("  Please download manually from ONNX Model Zoo")
        sys.exit(1)

    return str(MODEL_PATH)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STATISTICI
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def compute_stats(provider: str, latencies: list[float]) -> dict[str, Any]:
    """CalculeazÄƒ statistici din latenÈ›e."""
    return {
        'provider': provider,
        'mean_ms': np.mean(latencies),
        'std_ms': np.std(latencies),
        'p50_ms': np.percentile(latencies, 50),
        'p99_ms': np.percentile(latencies, 99),
        'min_ms': np.min(latencies),
        'max_ms': np.max(latencies),
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ONNX RUNTIME BENCHMARK
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def benchmark_onnxruntime(model_path: str, provider: str, iterations: int) -> dict:
    """Benchmark using ONNX Runtime."""
    import onnxruntime as ort

    try:
        sess = ort.InferenceSession(model_path, providers=[provider])
    except Exception as e:
        return {'provider': provider, 'error': str(e)}

    input_name = sess.get_inputs()[0].name
    input_shape = [1 if isinstance(d, str) else d for d in sess.get_inputs()[0].shape]
    input_data = {input_name: np.random.randn(*input_shape).astype(np.float32)}

    for _ in range(WARMUP_ITERATIONS):
        sess.run(None, input_data)

    latencies = []
    for _ in range(iterations):
        start = time.perf_counter()
        sess.run(None, input_data)
        latencies.append((time.perf_counter() - start) * 1000)

    return compute_stats(provider, latencies)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# OPENVINO BENCHMARK
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _run_openvino_inference(compiled, input_data: np.ndarray, iterations: int) -> list:
    """ExecutÄƒ inferenÈ›a OpenVINO È™i returneazÄƒ latenÈ›ele."""
    for _ in range(WARMUP_ITERATIONS):
        compiled([input_data])

    latencies = []
    for _ in range(iterations):
        start = time.perf_counter()
        compiled([input_data])
        latencies.append((time.perf_counter() - start) * 1000)
    return latencies


def benchmark_openvino(model_path: str, device: str, iterations: int) -> dict:
    """Benchmark using OpenVINO."""
    provider = f'OpenVINO-{device}'
    try:
        import openvino as ov
        core = ov.Core()

        if device not in core.available_devices:
            return {'provider': provider, 'error': f'Device {device} not available'}

        model = core.read_model(model_path)
        compiled = core.compile_model(model, device)
        input_shape = [1 if d == -1 else d for d in model.input().shape]
        input_data = np.random.randn(*input_shape).astype(np.float32)

        latencies = _run_openvino_inference(compiled, input_data, iterations)
        return compute_stats(provider, latencies)

    except Exception as e:
        return {'provider': provider, 'error': str(e)}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# AFIÈ˜ARE REZULTATE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def print_header() -> None:
    """AfiÈ™eazÄƒ header-ul programului."""
    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘            NPU vs CPU vs GPU Inference Benchmark                  â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    print()


def print_results_table(results: list[dict]) -> None:
    """Print benchmark results in a nice table."""
    print("\n" + "=" * 80)
    print(f"{'Provider':<25} {'Mean (ms)':<12} {'P50 (ms)':<12} {'P99 (ms)':<12} {'Std':<10}")
    print("=" * 80)

    for r in results:
        if 'error' in r:
            print(f"{r.get('provider', 'Unknown'):<25} ERROR: {r['error'][:40]}")
        else:
            print(f"{r['provider']:<25} {r['mean_ms']:<12.2f} {r['p50_ms']:<12.2f} "
                  f"{r['p99_ms']:<12.2f} {r['std_ms']:<10.2f}")

    print("=" * 80)


def print_fastest(results: list[dict]) -> None:
    """AfiÈ™eazÄƒ cel mai rapid provider."""
    valid = [r for r in results if 'error' not in r]
    if valid:
        fastest = min(valid, key=lambda x: x['mean_ms'])
        print(f"\nðŸ† Fastest: {fastest['provider']} ({fastest['mean_ms']:.2f} ms)")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# BENCHMARK RUNNERS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def run_onnx_benchmarks(model_path: str, iterations: int) -> list[dict]:
    """RuleazÄƒ benchmark-urile ONNX Runtime."""
    results = []
    print("Running ONNX Runtime benchmarks...")

    try:
        import onnxruntime as ort
        print(f"  ONNX Runtime version: {ort.__version__}")
        print(f"  Available providers: {ort.get_available_providers()}")

        print("  Testing CPU...")
        results.append(benchmark_onnxruntime(model_path, 'CPUExecutionProvider', iterations))

        if 'CUDAExecutionProvider' in ort.get_available_providers():
            print("  Testing CUDA GPU...")
            results.append(benchmark_onnxruntime(model_path, 'CUDAExecutionProvider', iterations))

        if 'DmlExecutionProvider' in ort.get_available_providers():
            print("  Testing DirectML...")
            results.append(benchmark_onnxruntime(model_path, 'DmlExecutionProvider', iterations))

    except ImportError:
        print("  ONNX Runtime not installed")

    return results


def run_openvino_benchmarks(model_path: str, iterations: int) -> list[dict]:
    """RuleazÄƒ benchmark-urile OpenVINO."""
    results = []
    print("\nRunning OpenVINO benchmarks...")

    try:
        import openvino as ov
        core = ov.Core()
        print(f"  OpenVINO version: {ov.__version__}")
        print(f"  Available devices: {core.available_devices}")

        for device in core.available_devices:
            print(f"  Testing {device}...")
            results.append(benchmark_openvino(model_path, device, iterations))

    except ImportError:
        print("  OpenVINO not installed (pip install openvino)")

    return results


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MAIN
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main() -> int:
    """Punct de intrare principal."""
    print_header()

    model_path = download_model()
    print(f"Model: {model_path}\n")

    results = []
    results.extend(run_onnx_benchmarks(model_path, DEFAULT_ITERATIONS))
    results.extend(run_openvino_benchmarks(model_path, DEFAULT_ITERATIONS))

    print_results_table(results)
    print_fastest(results)

    return 0


if __name__ == "__main__":
    raise SystemExit(main())
